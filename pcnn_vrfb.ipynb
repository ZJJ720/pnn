{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from DNN_tools import *\n",
    "import random\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import copy\n",
    "from physical_dc import *\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# 设置随机数种子\n",
    "setup_seed(27)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length  = 0.8\n",
    "batch_size    = 100\n",
    "num_epochs    = 200\n",
    "step_size     = 40 # 学习率变化周期\n",
    "gamma         = 0.2 # 学习率更新值\n",
    "learning_rate = 0.001 # 学习率\n",
    "\n",
    "# 模型输出迭代初始值\n",
    "S0            = 500\n",
    "k_p0          = 0.0001\n",
    "k_n0          = 0.00002\n",
    "theta_e0      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 dataset\n",
    "dataset_1 = create_dataset(1)\n",
    "dataset_2 = create_dataset(2)\n",
    "dataset_3 = create_dataset(3)\n",
    "dataset_4 = create_dataset(4)\n",
    "\n",
    "# 将数据的前 train_length 的数据作为训练集 train_set\n",
    "train_size_1 = int(dataset_1.shape[0] * train_length)\n",
    "train_size_2 = int(dataset_2.shape[0] * train_length)\n",
    "train_size_3 = int(dataset_3.shape[0] * train_length)\n",
    "train_size_4 = int(dataset_4.shape[0] * train_length)\n",
    "test_size = []\n",
    "test_size.append(dataset_1.shape[0] * (1 - train_length))\n",
    "test_size.append(dataset_2.shape[0] * (1 - train_length))\n",
    "test_size.append(dataset_3.shape[0] * (1 - train_length))\n",
    "test_size.append(dataset_4.shape[0] * (1 - train_length))\n",
    "\n",
    "# 分离出训练集和测试集\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_data.append(dataset_1[0:train_size_1])\n",
    "test_data.append(dataset_1[train_size_1:])\n",
    "train_data.append(dataset_2[0:train_size_2])\n",
    "test_data.append(dataset_2[train_size_2:])\n",
    "train_data.append(dataset_3[0:train_size_3])\n",
    "test_data.append(dataset_3[train_size_3:])\n",
    "train_data.append(dataset_4[0:train_size_4])\n",
    "test_data.append(dataset_4[train_size_4:])\n",
    "\n",
    "train = np.concatenate((train_data[0], train_data[1], train_data[2], train_data[3]), axis=0)\n",
    "np.random.shuffle(train)\n",
    "\n",
    "\n",
    "# 数据归一化\n",
    "scaler, train_scaled = train_scale(train)\n",
    "dataset = DataPrepare(train_scaled) # 设置 inputs 和 labels\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCNN for VRFB Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全连接神经网络类\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim=5, output_dim=4, hidden_layers=[256, 128, 64, 32, 16, 8]):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # 定义隐藏层\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        layer_sizes = [input_dim] + hidden_layers\n",
    "        for i in range(len(hidden_layers)):\n",
    "            self.hidden_layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            self.hidden_layers.append(nn.ReLU())  # 使用ReLU作为激活函数\n",
    "\n",
    "        # 定义输出层\n",
    "        self.output1_layer = nn.Linear(hidden_layers[-1], output_dim)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "        # self.output2_layer = nn.Linear(output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.hidden_layers:\n",
    "            x = linear(x)\n",
    "        out1 = self.output1_layer(self.sigmod(x))\n",
    "        # out2 = self.output2_layer(out1)\n",
    "        # return out1, out2\n",
    "        return out1\n",
    "\n",
    "\n",
    "# def physical_constraints(inputs, preds, labels):\n",
    "#     S = preds[:, 0]\n",
    "#     k_p = preds[:, 1]\n",
    "#     k_n = preds[:, 2]\n",
    "#     theta_e = preds[:, 3]\n",
    "#     S = S0 * torch.exp(S).cpu()\n",
    "#     k_p = k_p0 * torch.exp(k_p).cpu()\n",
    "#     k_n = k_n0 * torch.exp(k_n).cpu()\n",
    "#     theta_e = theta_e0 * torch.exp(theta_e).cpu()\n",
    "#     T, I, SoC, Q, C_V0, U = train_invert_scale(scaler, inputs.view(-1, 5).cpu().numpy(),\n",
    "#                                                labels.view(-1, 1).cpu().detach().numpy())\n",
    "#     C_2, C_3, C_4, C_5, C_Hn, C_Hp, C_H2Op = get_con(SoC, C_V0, C_Hp0, C_Hn0, C_H2Op0)\n",
    "#     e_con = E_con(T, I, Q, C_2, C_3, C_4, C_5)\n",
    "#     e_act = E_act(T, I, S, k_p, k_n, C_2, C_3, C_4, C_5)\n",
    "#     e_ohm = E_ohm(theta_e, T, I)\n",
    "#     e_ocv = E_ocv(T, C_2, C_3, C_4, C_5, C_Hp, C_Hn, C_H2Op)\n",
    "#     e_cell = e_con + e_act + e_ohm + e_ocv\n",
    "#     return e_cell, U\n",
    "\n",
    "\n",
    "# 创建模型实例\n",
    "model = DNN().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_function = nn.MSELoss()\n",
    "loss_function = loss_function.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam 优化器\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train_loss 0.01499264\n",
      "epoch 0 S 86.59621429 k_p 0.00001494 k_n 0.00000520 theta_e 507.41839600\n",
      "epoch 1 train_loss 0.00460094\n",
      "epoch 1 S 87.38838196 k_p 0.00001342 k_n 0.00000567 theta_e 271.91534424\n",
      "epoch 2 train_loss 0.00424167\n",
      "epoch 2 S 98.80868530 k_p 0.00001357 k_n 0.00000689 theta_e 101.97662354\n",
      "epoch 3 train_loss 0.00374706\n",
      "epoch 3 S 117.54332733 k_p 0.00001452 k_n 0.00000876 theta_e 59.42741776\n",
      "epoch 4 train_loss 0.00357555\n",
      "epoch 4 S 127.19138336 k_p 0.00001421 k_n 0.00001012 theta_e 51.26027298\n",
      "epoch 5 train_loss 0.00354289\n",
      "epoch 5 S 130.84219360 k_p 0.00001352 k_n 0.00001097 theta_e 49.12707520\n",
      "epoch 6 train_loss 0.00353416\n",
      "epoch 6 S 132.28623962 k_p 0.00001296 k_n 0.00001150 theta_e 48.50928879\n",
      "epoch 7 train_loss 0.00353134\n",
      "epoch 7 S 132.96691895 k_p 0.00001260 k_n 0.00001183 theta_e 48.31154633\n",
      "epoch 8 train_loss 0.00353050\n",
      "epoch 8 S 133.34535217 k_p 0.00001241 k_n 0.00001202 theta_e 48.24108505\n",
      "epoch 9 train_loss 0.00353032\n",
      "epoch 9 S 133.58215332 k_p 0.00001231 k_n 0.00001212 theta_e 48.21697617\n",
      "epoch 10 train_loss 0.00353034\n",
      "epoch 10 S 133.74774170 k_p 0.00001227 k_n 0.00001218 theta_e 48.21274948\n",
      "epoch 11 train_loss 0.00353041\n",
      "epoch 11 S 133.87768555 k_p 0.00001226 k_n 0.00001221 theta_e 48.21710968\n",
      "epoch 12 train_loss 0.00353050\n",
      "epoch 12 S 133.98902893 k_p 0.00001226 k_n 0.00001222 theta_e 48.22434616\n",
      "epoch 13 train_loss 0.00353059\n",
      "epoch 13 S 134.08901978 k_p 0.00001226 k_n 0.00001224 theta_e 48.23167038\n",
      "epoch 14 train_loss 0.00353068\n",
      "epoch 14 S 134.17976379 k_p 0.00001227 k_n 0.00001225 theta_e 48.23779678\n",
      "epoch 15 train_loss 0.00353077\n",
      "epoch 15 S 134.26162720 k_p 0.00001227 k_n 0.00001225 theta_e 48.24239731\n",
      "epoch 16 train_loss 0.00353086\n",
      "epoch 16 S 134.33445740 k_p 0.00001228 k_n 0.00001226 theta_e 48.24549484\n",
      "epoch 17 train_loss 0.00353093\n",
      "epoch 17 S 134.39825439 k_p 0.00001229 k_n 0.00001227 theta_e 48.24731827\n",
      "epoch 18 train_loss 0.00353100\n",
      "epoch 18 S 134.45332336 k_p 0.00001229 k_n 0.00001227 theta_e 48.24814606\n",
      "epoch 19 train_loss 0.00353107\n",
      "epoch 19 S 134.50032043 k_p 0.00001229 k_n 0.00001228 theta_e 48.24825287\n",
      "epoch 20 train_loss 0.00353112\n",
      "epoch 20 S 134.54003906 k_p 0.00001230 k_n 0.00001228 theta_e 48.24788666\n",
      "epoch 21 train_loss 0.00353117\n",
      "epoch 21 S 134.57322693 k_p 0.00001230 k_n 0.00001229 theta_e 48.24722290\n",
      "epoch 22 train_loss 0.00353121\n",
      "epoch 22 S 134.60084534 k_p 0.00001230 k_n 0.00001229 theta_e 48.24638367\n",
      "epoch 23 train_loss 0.00353124\n",
      "epoch 23 S 134.62370300 k_p 0.00001230 k_n 0.00001229 theta_e 48.24550629\n",
      "epoch 24 train_loss 0.00353127\n",
      "epoch 24 S 134.64250183 k_p 0.00001230 k_n 0.00001230 theta_e 48.24462509\n",
      "epoch 25 train_loss 0.00353129\n",
      "epoch 25 S 134.65794373 k_p 0.00001230 k_n 0.00001230 theta_e 48.24378967\n",
      "epoch 26 train_loss 0.00353131\n",
      "epoch 26 S 134.67054749 k_p 0.00001230 k_n 0.00001230 theta_e 48.24301910\n",
      "epoch 27 train_loss 0.00353133\n",
      "epoch 27 S 134.68084717 k_p 0.00001230 k_n 0.00001230 theta_e 48.24235535\n",
      "epoch 28 train_loss 0.00353134\n",
      "epoch 28 S 134.68928528 k_p 0.00001230 k_n 0.00001230 theta_e 48.24172974\n",
      "epoch 29 train_loss 0.00353135\n",
      "epoch 29 S 134.69610596 k_p 0.00001231 k_n 0.00001230 theta_e 48.24125671\n",
      "epoch 30 train_loss 0.00353136\n",
      "epoch 30 S 134.70166016 k_p 0.00001231 k_n 0.00001230 theta_e 48.24079895\n",
      "epoch 31 train_loss 0.00353137\n",
      "epoch 31 S 134.70619202 k_p 0.00001231 k_n 0.00001230 theta_e 48.24045563\n",
      "epoch 32 train_loss 0.00353137\n",
      "epoch 32 S 134.70991516 k_p 0.00001231 k_n 0.00001230 theta_e 48.24011612\n",
      "epoch 33 train_loss 0.00353138\n",
      "epoch 33 S 134.71290588 k_p 0.00001231 k_n 0.00001231 theta_e 48.23985672\n",
      "epoch 34 train_loss 0.00353138\n",
      "epoch 34 S 134.71533203 k_p 0.00001231 k_n 0.00001231 theta_e 48.23964691\n",
      "epoch 35 train_loss 0.00353139\n",
      "epoch 35 S 134.71733093 k_p 0.00001231 k_n 0.00001231 theta_e 48.23942947\n",
      "epoch 36 train_loss 0.00353139\n",
      "epoch 36 S 134.71893311 k_p 0.00001231 k_n 0.00001231 theta_e 48.23927689\n",
      "epoch 37 train_loss 0.00353139\n",
      "epoch 37 S 134.72023010 k_p 0.00001231 k_n 0.00001231 theta_e 48.23915100\n",
      "epoch 38 train_loss 0.00353139\n",
      "epoch 38 S 134.72120667 k_p 0.00001231 k_n 0.00001231 theta_e 48.23909760\n",
      "epoch 39 train_loss 0.00353139\n",
      "epoch 39 S 134.72207642 k_p 0.00001231 k_n 0.00001231 theta_e 48.23896790\n",
      "epoch 40 train_loss 0.00352757\n",
      "epoch 40 S 132.80142212 k_p 0.00001213 k_n 0.00001213 theta_e 47.48161697\n",
      "epoch 41 train_loss 0.00352788\n",
      "epoch 41 S 132.83628845 k_p 0.00001213 k_n 0.00001213 theta_e 47.45971680\n",
      "epoch 42 train_loss 0.00352788\n",
      "epoch 42 S 132.85995483 k_p 0.00001214 k_n 0.00001214 theta_e 47.44511414\n",
      "epoch 43 train_loss 0.00352788\n",
      "epoch 43 S 132.87605286 k_p 0.00001214 k_n 0.00001214 theta_e 47.43539047\n",
      "epoch 44 train_loss 0.00352789\n",
      "epoch 44 S 132.88688660 k_p 0.00001214 k_n 0.00001214 theta_e 47.42902756\n",
      "epoch 45 train_loss 0.00352789\n",
      "epoch 45 S 132.89427185 k_p 0.00001214 k_n 0.00001214 theta_e 47.42483902\n",
      "epoch 46 train_loss 0.00352789\n",
      "epoch 46 S 132.89923096 k_p 0.00001214 k_n 0.00001214 theta_e 47.42211533\n",
      "epoch 47 train_loss 0.00352789\n",
      "epoch 47 S 132.90267944 k_p 0.00001214 k_n 0.00001214 theta_e 47.42031097\n",
      "epoch 48 train_loss 0.00352789\n",
      "epoch 48 S 132.90501404 k_p 0.00001214 k_n 0.00001214 theta_e 47.41915131\n",
      "epoch 49 train_loss 0.00352789\n",
      "epoch 49 S 132.90657043 k_p 0.00001214 k_n 0.00001214 theta_e 47.41841125\n",
      "epoch 50 train_loss 0.00352790\n",
      "epoch 50 S 132.90766907 k_p 0.00001214 k_n 0.00001214 theta_e 47.41793442\n",
      "epoch 51 train_loss 0.00352790\n",
      "epoch 51 S 132.90840149 k_p 0.00001214 k_n 0.00001214 theta_e 47.41768646\n",
      "epoch 52 train_loss 0.00352790\n",
      "epoch 52 S 132.90892029 k_p 0.00001214 k_n 0.00001214 theta_e 47.41751862\n",
      "epoch 53 train_loss 0.00352790\n",
      "epoch 53 S 132.90933228 k_p 0.00001214 k_n 0.00001214 theta_e 47.41739655\n",
      "epoch 54 train_loss 0.00352790\n",
      "epoch 54 S 132.90960693 k_p 0.00001214 k_n 0.00001214 theta_e 47.41738129\n",
      "epoch 55 train_loss 0.00352790\n",
      "epoch 55 S 132.90980530 k_p 0.00001214 k_n 0.00001214 theta_e 47.41735458\n",
      "epoch 56 train_loss 0.00352790\n",
      "epoch 56 S 132.90997314 k_p 0.00001214 k_n 0.00001214 theta_e 47.41733551\n",
      "epoch 57 train_loss 0.00352790\n",
      "epoch 57 S 132.91009521 k_p 0.00001214 k_n 0.00001214 theta_e 47.41730499\n",
      "epoch 58 train_loss 0.00352790\n",
      "epoch 58 S 132.91017151 k_p 0.00001214 k_n 0.00001214 theta_e 47.41728973\n",
      "epoch 59 train_loss 0.00352790\n",
      "epoch 59 S 132.91024780 k_p 0.00001214 k_n 0.00001214 theta_e 47.41728210\n",
      "epoch 60 train_loss 0.00352790\n",
      "epoch 60 S 132.91030884 k_p 0.00001214 k_n 0.00001214 theta_e 47.41728973\n",
      "epoch 61 train_loss 0.00352790\n",
      "epoch 61 S 132.91038513 k_p 0.00001214 k_n 0.00001214 theta_e 47.41728210\n",
      "epoch 62 train_loss 0.00352790\n",
      "epoch 62 S 132.91040039 k_p 0.00001214 k_n 0.00001214 theta_e 47.41728973\n",
      "epoch 63 train_loss 0.00352790\n",
      "epoch 63 S 132.91038513 k_p 0.00001214 k_n 0.00001214 theta_e 47.41732025\n",
      "epoch 64 train_loss 0.00352790\n",
      "epoch 64 S 132.91038513 k_p 0.00001214 k_n 0.00001214 theta_e 47.41732025\n",
      "epoch 65 train_loss 0.00352790\n",
      "epoch 65 S 132.91040039 k_p 0.00001214 k_n 0.00001214 theta_e 47.41730499\n",
      "epoch 66 train_loss 0.00352790\n",
      "epoch 66 S 132.91038513 k_p 0.00001214 k_n 0.00001214 theta_e 47.41732025\n",
      "epoch 67 train_loss 0.00352790\n",
      "epoch 67 S 132.91038513 k_p 0.00001214 k_n 0.00001214 theta_e 47.41733551\n",
      "epoch 68 train_loss 0.00352790\n",
      "epoch 68 S 132.91038513 k_p 0.00001214 k_n 0.00001214 theta_e 47.41736221\n",
      "epoch 69 train_loss 0.00352790\n",
      "epoch 69 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41736221\n",
      "epoch 70 train_loss 0.00352790\n",
      "epoch 70 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41736221\n",
      "epoch 71 train_loss 0.00352790\n",
      "epoch 71 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41738129\n",
      "epoch 72 train_loss 0.00352790\n",
      "epoch 72 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41738129\n",
      "epoch 73 train_loss 0.00352790\n",
      "epoch 73 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41737366\n",
      "epoch 74 train_loss 0.00352790\n",
      "epoch 74 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41739655\n",
      "epoch 75 train_loss 0.00352790\n",
      "epoch 75 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41739655\n",
      "epoch 76 train_loss 0.00352790\n",
      "epoch 76 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41740417\n",
      "epoch 77 train_loss 0.00352790\n",
      "epoch 77 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41740417\n",
      "epoch 78 train_loss 0.00352790\n",
      "epoch 78 S 132.91041565 k_p 0.00001214 k_n 0.00001214 theta_e 47.41740417\n",
      "epoch 79 train_loss 0.00352790\n",
      "epoch 79 S 132.91046143 k_p 0.00001214 k_n 0.00001214 theta_e 47.41739655\n",
      "epoch 80 train_loss 0.00352720\n",
      "epoch 80 S 133.01225281 k_p 0.00001215 k_n 0.00001215 theta_e 47.44977570\n",
      "epoch 81 train_loss 0.00352714\n",
      "epoch 81 S 133.02198792 k_p 0.00001215 k_n 0.00001215 theta_e 47.45092773\n",
      "epoch 82 train_loss 0.00352713\n",
      "epoch 82 S 133.02467346 k_p 0.00001215 k_n 0.00001215 theta_e 47.44976044\n",
      "epoch 83 train_loss 0.00352713\n",
      "epoch 83 S 133.02651978 k_p 0.00001215 k_n 0.00001215 theta_e 47.44849014\n",
      "epoch 84 train_loss 0.00352713\n",
      "epoch 84 S 133.02816772 k_p 0.00001215 k_n 0.00001215 theta_e 47.44726944\n",
      "epoch 85 train_loss 0.00352713\n",
      "epoch 85 S 133.02964783 k_p 0.00001215 k_n 0.00001215 theta_e 47.44609451\n",
      "epoch 86 train_loss 0.00352713\n",
      "epoch 86 S 133.03106689 k_p 0.00001215 k_n 0.00001215 theta_e 47.44506454\n",
      "epoch 87 train_loss 0.00352713\n",
      "epoch 87 S 133.03245544 k_p 0.00001215 k_n 0.00001215 theta_e 47.44408798\n",
      "epoch 88 train_loss 0.00352713\n",
      "epoch 88 S 133.03370667 k_p 0.00001215 k_n 0.00001215 theta_e 47.44318771\n",
      "epoch 89 train_loss 0.00352713\n",
      "epoch 89 S 133.03482056 k_p 0.00001215 k_n 0.00001215 theta_e 47.44235992\n",
      "epoch 90 train_loss 0.00352713\n",
      "epoch 90 S 133.03588867 k_p 0.00001215 k_n 0.00001215 theta_e 47.44157791\n",
      "epoch 91 train_loss 0.00352713\n",
      "epoch 91 S 133.03688049 k_p 0.00001215 k_n 0.00001215 theta_e 47.44089508\n",
      "epoch 92 train_loss 0.00352713\n",
      "epoch 92 S 133.03779602 k_p 0.00001215 k_n 0.00001215 theta_e 47.44025421\n",
      "epoch 93 train_loss 0.00352713\n",
      "epoch 93 S 133.03863525 k_p 0.00001215 k_n 0.00001215 theta_e 47.43964386\n",
      "epoch 94 train_loss 0.00352713\n",
      "epoch 94 S 133.03945923 k_p 0.00001215 k_n 0.00001215 theta_e 47.43912125\n",
      "epoch 95 train_loss 0.00352713\n",
      "epoch 95 S 133.04011536 k_p 0.00001215 k_n 0.00001215 theta_e 47.43864441\n",
      "epoch 96 train_loss 0.00352713\n",
      "epoch 96 S 133.04075623 k_p 0.00001215 k_n 0.00001215 theta_e 47.43821716\n",
      "epoch 97 train_loss 0.00352713\n",
      "epoch 97 S 133.04132080 k_p 0.00001215 k_n 0.00001215 theta_e 47.43779373\n",
      "epoch 98 train_loss 0.00352713\n",
      "epoch 98 S 133.04185486 k_p 0.00001215 k_n 0.00001215 theta_e 47.43742371\n",
      "epoch 99 train_loss 0.00352713\n",
      "epoch 99 S 133.04232788 k_p 0.00001215 k_n 0.00001215 theta_e 47.43708420\n",
      "epoch 100 train_loss 0.00352713\n",
      "epoch 100 S 133.04281616 k_p 0.00001215 k_n 0.00001215 theta_e 47.43675232\n",
      "epoch 101 train_loss 0.00352713\n",
      "epoch 101 S 133.04322815 k_p 0.00001215 k_n 0.00001215 theta_e 47.43647766\n",
      "epoch 102 train_loss 0.00352713\n",
      "epoch 102 S 133.04362488 k_p 0.00001215 k_n 0.00001215 theta_e 47.43617249\n",
      "epoch 103 train_loss 0.00352713\n",
      "epoch 103 S 133.04400635 k_p 0.00001215 k_n 0.00001215 theta_e 47.43592834\n",
      "epoch 104 train_loss 0.00352713\n",
      "epoch 104 S 133.04434204 k_p 0.00001215 k_n 0.00001215 theta_e 47.43569183\n",
      "epoch 105 train_loss 0.00352713\n",
      "epoch 105 S 133.04466248 k_p 0.00001215 k_n 0.00001215 theta_e 47.43546295\n",
      "epoch 106 train_loss 0.00352713\n",
      "epoch 106 S 133.04492188 k_p 0.00001215 k_n 0.00001215 theta_e 47.43526459\n",
      "epoch 107 train_loss 0.00352713\n",
      "epoch 107 S 133.04518127 k_p 0.00001215 k_n 0.00001215 theta_e 47.43509293\n",
      "epoch 108 train_loss 0.00352713\n",
      "epoch 108 S 133.04542542 k_p 0.00001215 k_n 0.00001215 theta_e 47.43493271\n",
      "epoch 109 train_loss 0.00352713\n",
      "epoch 109 S 133.04566956 k_p 0.00001215 k_n 0.00001215 theta_e 47.43476868\n",
      "epoch 110 train_loss 0.00352713\n",
      "epoch 110 S 133.04589844 k_p 0.00001215 k_n 0.00001215 theta_e 47.43461609\n",
      "epoch 111 train_loss 0.00352713\n",
      "epoch 111 S 133.04608154 k_p 0.00001215 k_n 0.00001215 theta_e 47.43449402\n",
      "epoch 112 train_loss 0.00352713\n",
      "epoch 112 S 133.04623413 k_p 0.00001215 k_n 0.00001215 theta_e 47.43437195\n",
      "epoch 113 train_loss 0.00352713\n",
      "epoch 113 S 133.04640198 k_p 0.00001215 k_n 0.00001215 theta_e 47.43425369\n",
      "epoch 114 train_loss 0.00352713\n",
      "epoch 114 S 133.04653931 k_p 0.00001215 k_n 0.00001215 theta_e 47.43413544\n",
      "epoch 115 train_loss 0.00352713\n",
      "epoch 115 S 133.04669189 k_p 0.00001215 k_n 0.00001215 theta_e 47.43403625\n",
      "epoch 116 train_loss 0.00352713\n",
      "epoch 116 S 133.04678345 k_p 0.00001215 k_n 0.00001215 theta_e 47.43395615\n",
      "epoch 117 train_loss 0.00352713\n",
      "epoch 117 S 133.04689026 k_p 0.00001215 k_n 0.00001215 theta_e 47.43388367\n",
      "epoch 118 train_loss 0.00352713\n",
      "epoch 118 S 133.04701233 k_p 0.00001215 k_n 0.00001215 theta_e 47.43378830\n",
      "epoch 119 train_loss 0.00352713\n",
      "epoch 119 S 133.04707336 k_p 0.00001215 k_n 0.00001215 theta_e 47.43371964\n",
      "epoch 120 train_loss 0.00352697\n",
      "epoch 120 S 133.10594177 k_p 0.00001216 k_n 0.00001216 theta_e 47.45454407\n",
      "epoch 121 train_loss 0.00352691\n",
      "epoch 121 S 133.14593506 k_p 0.00001216 k_n 0.00001216 theta_e 47.46880341\n",
      "epoch 122 train_loss 0.00352688\n",
      "epoch 122 S 133.17269897 k_p 0.00001217 k_n 0.00001217 theta_e 47.47835541\n",
      "epoch 123 train_loss 0.00352687\n",
      "epoch 123 S 133.19065857 k_p 0.00001217 k_n 0.00001217 theta_e 47.48474121\n",
      "epoch 124 train_loss 0.00352686\n",
      "epoch 124 S 133.20268250 k_p 0.00001217 k_n 0.00001217 theta_e 47.48903275\n",
      "epoch 125 train_loss 0.00352686\n",
      "epoch 125 S 133.21073914 k_p 0.00001217 k_n 0.00001217 theta_e 47.49188995\n",
      "epoch 126 train_loss 0.00352685\n",
      "epoch 126 S 133.21615601 k_p 0.00001217 k_n 0.00001217 theta_e 47.49372101\n",
      "epoch 127 train_loss 0.00352685\n",
      "epoch 127 S 133.21980286 k_p 0.00001217 k_n 0.00001217 theta_e 47.49487686\n",
      "epoch 128 train_loss 0.00352685\n",
      "epoch 128 S 133.22225952 k_p 0.00001217 k_n 0.00001217 theta_e 47.49571228\n",
      "epoch 129 train_loss 0.00352685\n",
      "epoch 129 S 133.22402954 k_p 0.00001217 k_n 0.00001217 theta_e 47.49623871\n",
      "epoch 130 train_loss 0.00352685\n",
      "epoch 130 S 133.22518921 k_p 0.00001217 k_n 0.00001217 theta_e 47.49650574\n",
      "epoch 131 train_loss 0.00352685\n",
      "epoch 131 S 133.22602844 k_p 0.00001217 k_n 0.00001217 theta_e 47.49670029\n",
      "epoch 132 train_loss 0.00352685\n",
      "epoch 132 S 133.22665405 k_p 0.00001217 k_n 0.00001217 theta_e 47.49680328\n",
      "epoch 133 train_loss 0.00352685\n",
      "epoch 133 S 133.22706604 k_p 0.00001217 k_n 0.00001217 theta_e 47.49682999\n",
      "epoch 134 train_loss 0.00352685\n",
      "epoch 134 S 133.22737122 k_p 0.00001217 k_n 0.00001217 theta_e 47.49684525\n",
      "epoch 135 train_loss 0.00352685\n",
      "epoch 135 S 133.22761536 k_p 0.00001217 k_n 0.00001217 theta_e 47.49682999\n",
      "epoch 136 train_loss 0.00352685\n",
      "epoch 136 S 133.22779846 k_p 0.00001217 k_n 0.00001217 theta_e 47.49682617\n",
      "epoch 137 train_loss 0.00352685\n",
      "epoch 137 S 133.22795105 k_p 0.00001217 k_n 0.00001217 theta_e 47.49680328\n",
      "epoch 138 train_loss 0.00352685\n",
      "epoch 138 S 133.22804260 k_p 0.00001217 k_n 0.00001217 theta_e 47.49677277\n",
      "epoch 139 train_loss 0.00352685\n",
      "epoch 139 S 133.22817993 k_p 0.00001217 k_n 0.00001217 theta_e 47.49670792\n",
      "epoch 140 train_loss 0.00352685\n",
      "epoch 140 S 133.22825623 k_p 0.00001217 k_n 0.00001217 theta_e 47.49670029\n",
      "epoch 141 train_loss 0.00352685\n",
      "epoch 141 S 133.22834778 k_p 0.00001217 k_n 0.00001217 theta_e 47.49665451\n",
      "epoch 142 train_loss 0.00352685\n",
      "epoch 142 S 133.22842407 k_p 0.00001217 k_n 0.00001217 theta_e 47.49660492\n",
      "epoch 143 train_loss 0.00352685\n",
      "epoch 143 S 133.22850037 k_p 0.00001217 k_n 0.00001217 theta_e 47.49656296\n",
      "epoch 144 train_loss 0.00352685\n",
      "epoch 144 S 133.22859192 k_p 0.00001217 k_n 0.00001217 theta_e 47.49651337\n",
      "epoch 145 train_loss 0.00352685\n",
      "epoch 145 S 133.22860718 k_p 0.00001217 k_n 0.00001217 theta_e 47.49647522\n",
      "epoch 146 train_loss 0.00352685\n",
      "epoch 146 S 133.22868347 k_p 0.00001217 k_n 0.00001217 theta_e 47.49643707\n",
      "epoch 147 train_loss 0.00352685\n",
      "epoch 147 S 133.22874451 k_p 0.00001217 k_n 0.00001217 theta_e 47.49639130\n",
      "epoch 148 train_loss 0.00352685\n",
      "epoch 148 S 133.22880554 k_p 0.00001217 k_n 0.00001217 theta_e 47.49635696\n",
      "epoch 149 train_loss 0.00352685\n",
      "epoch 149 S 133.22886658 k_p 0.00001217 k_n 0.00001217 theta_e 47.49633408\n",
      "epoch 150 train_loss 0.00352685\n",
      "epoch 150 S 133.22892761 k_p 0.00001217 k_n 0.00001217 theta_e 47.49628448\n",
      "epoch 151 train_loss 0.00352685\n",
      "epoch 151 S 133.22898865 k_p 0.00001217 k_n 0.00001217 theta_e 47.49625015\n",
      "epoch 152 train_loss 0.00352685\n",
      "epoch 152 S 133.22901917 k_p 0.00001217 k_n 0.00001217 theta_e 47.49619293\n",
      "epoch 153 train_loss 0.00352685\n",
      "epoch 153 S 133.22909546 k_p 0.00001217 k_n 0.00001217 theta_e 47.49615097\n",
      "epoch 154 train_loss 0.00352685\n",
      "epoch 154 S 133.22917175 k_p 0.00001217 k_n 0.00001217 theta_e 47.49610901\n",
      "epoch 155 train_loss 0.00352685\n",
      "epoch 155 S 133.22923279 k_p 0.00001217 k_n 0.00001217 theta_e 47.49608994\n",
      "epoch 156 train_loss 0.00352685\n",
      "epoch 156 S 133.22926331 k_p 0.00001217 k_n 0.00001217 theta_e 47.49604416\n",
      "epoch 157 train_loss 0.00352685\n",
      "epoch 157 S 133.22933960 k_p 0.00001217 k_n 0.00001217 theta_e 47.49600983\n",
      "epoch 158 train_loss 0.00352685\n",
      "epoch 158 S 133.22940063 k_p 0.00001217 k_n 0.00001217 theta_e 47.49597549\n",
      "epoch 159 train_loss 0.00352685\n",
      "epoch 159 S 133.22944641 k_p 0.00001217 k_n 0.00001217 theta_e 47.49594879\n",
      "epoch 160 train_loss 0.00352679\n",
      "epoch 160 S 133.23178101 k_p 0.00001217 k_n 0.00001217 theta_e 47.49674225\n",
      "epoch 161 train_loss 0.00352679\n",
      "epoch 161 S 133.23443604 k_p 0.00001217 k_n 0.00001217 theta_e 47.49766541\n",
      "epoch 162 train_loss 0.00352679\n",
      "epoch 162 S 133.23683167 k_p 0.00001217 k_n 0.00001217 theta_e 47.49858093\n",
      "epoch 163 train_loss 0.00352679\n",
      "epoch 163 S 133.23902893 k_p 0.00001217 k_n 0.00001217 theta_e 47.49936676\n",
      "epoch 164 train_loss 0.00352679\n",
      "epoch 164 S 133.24105835 k_p 0.00001217 k_n 0.00001217 theta_e 47.50010681\n",
      "epoch 165 train_loss 0.00352679\n",
      "epoch 165 S 133.24293518 k_p 0.00001217 k_n 0.00001217 theta_e 47.50077820\n",
      "epoch 166 train_loss 0.00352678\n",
      "epoch 166 S 133.24464417 k_p 0.00001217 k_n 0.00001217 theta_e 47.50138855\n",
      "epoch 167 train_loss 0.00352678\n",
      "epoch 167 S 133.24629211 k_p 0.00001217 k_n 0.00001217 theta_e 47.50191498\n",
      "epoch 168 train_loss 0.00352678\n",
      "epoch 168 S 133.24775696 k_p 0.00001217 k_n 0.00001217 theta_e 47.50242233\n",
      "epoch 169 train_loss 0.00352678\n",
      "epoch 169 S 133.24911499 k_p 0.00001217 k_n 0.00001217 theta_e 47.50288391\n",
      "epoch 170 train_loss 0.00352678\n",
      "epoch 170 S 133.25035095 k_p 0.00001217 k_n 0.00001217 theta_e 47.50331116\n",
      "epoch 171 train_loss 0.00352678\n",
      "epoch 171 S 133.25148010 k_p 0.00001217 k_n 0.00001217 theta_e 47.50371552\n",
      "epoch 172 train_loss 0.00352678\n",
      "epoch 172 S 133.25259399 k_p 0.00001217 k_n 0.00001217 theta_e 47.50408173\n",
      "epoch 173 train_loss 0.00352678\n",
      "epoch 173 S 133.25357056 k_p 0.00001217 k_n 0.00001217 theta_e 47.50444412\n",
      "epoch 174 train_loss 0.00352678\n",
      "epoch 174 S 133.25448608 k_p 0.00001217 k_n 0.00001217 theta_e 47.50476456\n",
      "epoch 175 train_loss 0.00352678\n",
      "epoch 175 S 133.25535583 k_p 0.00001217 k_n 0.00001217 theta_e 47.50505066\n",
      "epoch 176 train_loss 0.00352678\n",
      "epoch 176 S 133.25614929 k_p 0.00001217 k_n 0.00001217 theta_e 47.50529480\n",
      "epoch 177 train_loss 0.00352678\n",
      "epoch 177 S 133.25689697 k_p 0.00001217 k_n 0.00001217 theta_e 47.50551605\n",
      "epoch 178 train_loss 0.00352678\n",
      "epoch 178 S 133.25761414 k_p 0.00001217 k_n 0.00001217 theta_e 47.50576019\n",
      "epoch 179 train_loss 0.00352678\n",
      "epoch 179 S 133.25820923 k_p 0.00001217 k_n 0.00001217 theta_e 47.50598526\n",
      "epoch 180 train_loss 0.00352678\n",
      "epoch 180 S 133.25877380 k_p 0.00001217 k_n 0.00001217 theta_e 47.50617599\n",
      "epoch 181 train_loss 0.00352678\n",
      "epoch 181 S 133.25929260 k_p 0.00001217 k_n 0.00001217 theta_e 47.50636292\n",
      "epoch 182 train_loss 0.00352678\n",
      "epoch 182 S 133.25976562 k_p 0.00001217 k_n 0.00001217 theta_e 47.50653458\n",
      "epoch 183 train_loss 0.00352678\n",
      "epoch 183 S 133.26023865 k_p 0.00001217 k_n 0.00001217 theta_e 47.50666046\n",
      "epoch 184 train_loss 0.00352678\n",
      "epoch 184 S 133.26069641 k_p 0.00001217 k_n 0.00001217 theta_e 47.50679016\n",
      "epoch 185 train_loss 0.00352678\n",
      "epoch 185 S 133.26107788 k_p 0.00001217 k_n 0.00001217 theta_e 47.50691223\n",
      "epoch 186 train_loss 0.00352678\n",
      "epoch 186 S 133.26145935 k_p 0.00001217 k_n 0.00001217 theta_e 47.50703049\n",
      "epoch 187 train_loss 0.00352678\n",
      "epoch 187 S 133.26181030 k_p 0.00001217 k_n 0.00001217 theta_e 47.50711823\n",
      "epoch 188 train_loss 0.00352678\n",
      "epoch 188 S 133.26211548 k_p 0.00001217 k_n 0.00001217 theta_e 47.50719833\n",
      "epoch 189 train_loss 0.00352678\n",
      "epoch 189 S 133.26237488 k_p 0.00001217 k_n 0.00001217 theta_e 47.50725174\n",
      "epoch 190 train_loss 0.00352678\n",
      "epoch 190 S 133.26268005 k_p 0.00001217 k_n 0.00001217 theta_e 47.50732422\n",
      "epoch 191 train_loss 0.00352678\n",
      "epoch 191 S 133.26292419 k_p 0.00001217 k_n 0.00001217 theta_e 47.50737381\n",
      "epoch 192 train_loss 0.00352678\n",
      "epoch 192 S 133.26309204 k_p 0.00001217 k_n 0.00001217 theta_e 47.50744247\n",
      "epoch 193 train_loss 0.00352678\n",
      "epoch 193 S 133.26327515 k_p 0.00001217 k_n 0.00001217 theta_e 47.50745010\n",
      "epoch 194 train_loss 0.00352678\n",
      "epoch 194 S 133.26347351 k_p 0.00001217 k_n 0.00001217 theta_e 47.50749207\n",
      "epoch 195 train_loss 0.00352678\n",
      "epoch 195 S 133.26362610 k_p 0.00001217 k_n 0.00001217 theta_e 47.50751114\n",
      "epoch 196 train_loss 0.00352678\n",
      "epoch 196 S 133.26382446 k_p 0.00001217 k_n 0.00001217 theta_e 47.50753403\n",
      "epoch 197 train_loss 0.00352678\n",
      "epoch 197 S 133.26397705 k_p 0.00001217 k_n 0.00001217 theta_e 47.50756836\n",
      "epoch 198 train_loss 0.00352678\n",
      "epoch 198 S 133.26411438 k_p 0.00001217 k_n 0.00001217 theta_e 47.50758362\n",
      "epoch 199 train_loss 0.00352678\n",
      "epoch 199 S 133.26422119 k_p 0.00001217 k_n 0.00001217 theta_e 47.50759125\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "min_epochs = 10\n",
    "best_model = None\n",
    "min_loss = 0.2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.reshape(batch_size, 5)\n",
    "\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(torch.float32)\n",
    "        labels = labels.to(torch.float32)\n",
    "\n",
    "        # 前向传播\n",
    "        # preds1, preds2 = model(inputs)\n",
    "        preds = model(inputs)\n",
    "\n",
    "        # 计算损失\n",
    "        # e_cell, U = physical_constraints(inputs, preds, labels)\n",
    "        S = preds[:, 0]\n",
    "        k_p = preds[:, 1]\n",
    "        k_n = preds[:, 2]\n",
    "        theta_e = preds[:, 3]\n",
    "        S = S0 * torch.exp(S).cpu()\n",
    "        k_p = k_p0 * torch.exp(k_p).cpu()\n",
    "        k_n = k_n0 * torch.exp(k_n).cpu()\n",
    "        theta_e = theta_e0 * torch.exp(theta_e).cpu()\n",
    "        T, I, SoC, Q, C_V0, U = train_invert_scale(scaler, inputs.view(-1, 5).cpu().numpy(),\n",
    "                                                labels.view(-1, 1).cpu().detach().numpy())\n",
    "        C_2, C_3, C_4, C_5, C_Hn, C_Hp, C_H2Op = get_con(SoC, C_V0, C_Hp0, C_Hn0, C_H2Op0)\n",
    "        e_con = E_con(T, I, Q, C_2, C_3, C_4, C_5)\n",
    "        e_act = E_act(T, I, S, k_p, k_n, C_2, C_3, C_4, C_5)\n",
    "        e_ohm = E_ohm(theta_e, T, I)\n",
    "        e_ocv = E_ocv(T, C_2, C_3, C_4, C_5, C_Hp, C_Hn, C_H2Op)\n",
    "        e_cell = e_con + e_act + e_ohm + e_ocv\n",
    "        \n",
    "        loss = loss_function(e_cell, U)\n",
    "        train_loss.append(loss.cpu().item())\n",
    "\n",
    "        # 更新梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 优化参数\n",
    "        optimizer.step()  # 更新每个网络的权重\n",
    "\n",
    "    scheduler.step()  # 调整学习率\n",
    "\n",
    "    if epoch > min_epochs and loss < min_loss:\n",
    "        min_val_loss = loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    print('epoch {} train_loss {:.8f}'.format(epoch, np.mean(train_loss)))\n",
    "\n",
    "    S = S.mean().item()\n",
    "    k_p = k_p.mean().item()\n",
    "    k_n = k_n.mean().item()\n",
    "    theta_e = theta_e.mean().item()\n",
    "    print('epoch {} S {:.8f} k_p {:.8f} k_n {:.8f} theta_e {:.8f}'.format(epoch, S, k_p, k_n, theta_e))\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    torch.save(model, f'./result/DNN_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjjtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
